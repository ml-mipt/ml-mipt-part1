{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 14pt\">MIPT, Advanced ML, Spring 2018</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Organization Info</h1> \n",
    "\n",
    "* Дедлайн **18 апреля 2018 02:59** для всех групп.\n",
    "* В качестве решения задания нужно прислать ноутбук с подробными комментариями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оформление дз**: \n",
    "- Присылайте выполненное задание на почту ``ml.course.mipt@gmail.com``\n",
    "- Укажите тему письма в следующем формате ``ML2018_fall_<номер_группы>_<фамилия>``, к примеру -- ``ML2018_fall_495_ivanov``\n",
    "- Выполненное дз сохраните в файл ``<фамилия>_<группа>_task<номер>.ipnb, к примеру`` -- ``ivanov_401_task6.ipnb``\n",
    "\n",
    "**Вопросы**:\n",
    "- Присылайте вопросы на почту ``ml.course.mipt@gmail.com``\n",
    "- Укажите тему письма в следующем формате ``ML2018_fall Question <Содержание вопроса>``\n",
    "\n",
    "\n",
    "--------\n",
    "- **PS1:** Используются автоматические фильтры, и просто не найдем ваше дз, если вы неаккуратно его подпишите.\n",
    "- **PS2:**  Просроченный дедлайн снижает максимальный вес задания по формуле, указнной на первом семинаре\n",
    "- **PS3:** Допустимы исправление кода предложенного кода ниже, если вы считаете"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home work 1: Basic Artificial Neural Networks\n",
    "\n",
    "Credit https://github.com/yandexdataschool/YSDA_deeplearning17, https://github.com/DmitryUlyanov\n",
    "\n",
    "\n",
    "Зачем это всё нужно?! Зачем понимать как работают нейросети внутри когда уже есть куча библиотек?\n",
    "- Время от времени Ваши сети не учатся, веса становятся nan-ами, все расходится и разваливается -- это можно починить если понимать бекпроп\n",
    "- Если Вы не понимаете как работают оптимизаторы, то не сможете правильно выставить гиперпарааметры :) и тоже ничего выучить не выйдет\n",
    "- https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this homework is simple, yet an actual implementation may take some time :). We are going to write an Artificial Neural Network (almost) from scratch. The software design of was heavily inspired by [Torch](http://torch.ch) which is the most convenient neural network environment when the work involves defining new layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework requires sending **\"multiple** files, please do not forget to include all the files when sending to TA. The list of files:\n",
    "- This notebook\n",
    "- hw6_Modules.ipynb\n",
    "\n",
    "If you want to read more about backprop this links can be helpfull:\n",
    "- http://udacity.com/course/deep-learning--ud730 \n",
    "- http://cs231n.stanford.edu/2016/syllabus.html \n",
    "- http://www.deeplearningbook.org "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Check Questions</h1> \n",
    "\n",
    "**Вопрос 1**: Чем нейросети отличаются от линейных моделей а чем похожи? \n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 2**: В чем недостатки полносвзяных нейронных сетей какая мотивация к использованию свёрточных?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 3**: Какие слои используются в современных нейронных сетях? Опишите как работает каждый слой и свою интуицию зачем он нужен.\n",
    "\n",
    "    - DenceLayer -- линейное преобразование Wx + b, можно интерпретировать как ..., а так-же хорошо подходит тут ..., очень похож на линейную алгебру и по этому работает так ...\n",
    "    - <Тут Ваш слой>\n",
    "    - <Тут Ваш слой>\n",
    "    - <Тут Ваш слой>\n",
    "    - <Тут Ваш слой>\n",
    "    \n",
    "    \n",
    "**Вопрос 4**: Может ли нейросеть решать задачу регрессии, какой компонент для этого нужно заменить в нейросети из лекции 1?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "\n",
    "**Вопрос 5**: Почему обычные методы оптимизации плохо работают с нейросетями? А какие работают хорошо? Почему они работают хорошо?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 6**: Для чего нужен backprop, чем это лучше/хуже чем считать градиенты без него? Почему backprop эффективно считается на GPU?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 7**: Почему для нейросетей не используют кросс валидацию, что вместо неё? Можно-ли ее использовать?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "**Вопрос 8**: Небольшой quiz который поможет разобраться со свертками https://www.youtube.com/watch?v=DDRa5ASNdq4 \n",
    "\n",
    "<img src=\"conv.png\" width=\"600\">\n",
    "\n",
    "<Ответ-Картинка :)>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "Политика списывания. Вы можете обсудить решение с одногрупниками, так интереснее и веселее :) \n",
    "Не шарьте друг-другу код, в этом случаи вы ничему не научитесь -- \"мыши плакали кололись но продолжали жрать кактус\".\n",
    "\n",
    "Теперь формально. Разница между списыванием и помощью товарища иногда едва различима. Мы искренне надеемся, что при любых сложностях вы можете обратиться к семинаристам и с их подсказками **самостоятельно** справиться с заданием. При зафиксированных случаях списывания (одинаковый код, одинаковые ошибки), баллы за задание будут обнулены всем участникам инцидента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Важно \n",
    "\n",
    "    - Не забывайте делать GradCheck, чтобы проверить численно что производные правильные, обычно с первого раза не выходит никогда,   пример тут https://goo.gl/pzvzfe \n",
    "    - Ваш код не должен содержать циклов, все вычисления должны бить векторные, внутри numpy \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement everything in `Modules.ipynb`. Read all the comments thoughtfully to ease the pain. Please try not to change the prototypes.\n",
    "\n",
    "Do not forget, that each module should return AND store `output` and `gradInput`.\n",
    "\n",
    "The typical assumption is that `module.backward` is always executed after `module.forward`,\n",
    "so `output` is stored, this would be useful for `SoftMax`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    --------------------------------------\n",
    "    -- Tech note\n",
    "    --------------------------------------\n",
    "    Inspired by torch I would use\n",
    "    \n",
    "    np.multiply, np.add, np.divide, np.subtract instead of *,+,/,-\n",
    "    for better memory handling\n",
    "        \n",
    "    Suppose you allocated a variable    \n",
    "        \n",
    "        a = np.zeros(...)\n",
    "    \n",
    "    So, instead of\n",
    "    \n",
    "        a = b + c  # will be reallocated, GC needed to free\n",
    "    \n",
    "    I would go for: \n",
    "    \n",
    "        np.add(b,c,out = a) # puts result in `a`\n",
    "    \n",
    "    But it is completely up to you.\n",
    "\"\"\"\n",
    "%run hw6_Modules.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer is implemented for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_momentum(x, dx, config, state):\n",
    "    \"\"\"\n",
    "        This is a very ugly implementation of sgd with momentum \n",
    "        just to show an example how to store old grad in state.\n",
    "        \n",
    "        config:\n",
    "            - momentum\n",
    "            - learning_rate\n",
    "        state:\n",
    "            - old_grad\n",
    "    \"\"\"\n",
    "    \n",
    "    # x and dx have complex structure, old dx will be stored in a simpler one\n",
    "    state.setdefault('old_grad', {})\n",
    "    \n",
    "    i = 0 \n",
    "    for cur_layer_x, cur_layer_dx in zip(x,dx): \n",
    "        for cur_x, cur_dx in zip(cur_layer_x,cur_layer_dx):\n",
    "            \n",
    "            cur_old_grad = state['old_grad'].setdefault(i, np.zeros_like(cur_dx))\n",
    "            \n",
    "            np.add(config['momentum'] * cur_old_grad, config['learning_rate'] * cur_dx, out = cur_old_grad)\n",
    "            \n",
    "            cur_x -= cur_old_grad\n",
    "            i += 1     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this example to debug your code, start with logistic regression and then test other layers. You do not need to change anything here. This code is provided for you to test the layers. Also it is easy to use this code in MNIST task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "N = 500\n",
    "\n",
    "X1 = np.random.randn(N,2) + np.array([2,2])\n",
    "X2 = np.random.randn(N,2) + np.array([-2,-2])\n",
    "\n",
    "Y = np.concatenate([np.ones(N),np.zeros(N)])[:,None]\n",
    "Y = np.hstack([Y, 1-Y])\n",
    "\n",
    "X = np.vstack([X1,X2])\n",
    "plt.scatter(X[:,0],X[:,1], c = Y[:,0], edgecolors= 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a **logistic regression** for debugging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Sequential()\n",
    "net.add(Linear(2, 2))\n",
    "net.add(SoftMax())\n",
    "\n",
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "print(net)\n",
    "\n",
    "# Test something like that then \n",
    "\n",
    "# net = Sequential()\n",
    "# net.add(Linear(2, 4))\n",
    "# net.add(ReLU())\n",
    "# net.add(Linear(4, 2))\n",
    "# net.add(SoftMax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with batch_size = 1000 to make sure every step lowers the loss, then try stochastic version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iptimizer params\n",
    "optimizer_config = {'learning_rate' : 1e-1, 'momentum': 0.9}\n",
    "optimizer_state = {}\n",
    "\n",
    "# Looping params\n",
    "n_epoch = 20\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch generator\n",
    "def get_batches(dataset, batch_size):\n",
    "    X, Y = dataset\n",
    "    n_samples = X.shape[0]\n",
    "        \n",
    "    # Shuffle at the start of epoch\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_idx = indices[start:end]\n",
    "    \n",
    "        yield X[batch_idx], Y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic training loop. Examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for x_batch, y_batch in get_batches((X, Y), batch_size):\n",
    "        \n",
    "        net.zeroGradParameters()\n",
    "        \n",
    "        # Forward\n",
    "        predictions = net.forward(x_batch)\n",
    "        loss = criterion.forward(predictions, y_batch)\n",
    "    \n",
    "        # Backward\n",
    "        dp = criterion.backward(predictions, y_batch)\n",
    "        net.backward(x_batch, dp)\n",
    "        \n",
    "        # Update weights\n",
    "        sgd_momentum(net.getParameters(), \n",
    "                     net.getGradParameters(), \n",
    "                     optimizer_config,\n",
    "                     optimizer_state)      \n",
    "        \n",
    "        loss_history.append(loss)\n",
    "\n",
    "    # Visualize\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "        \n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"#iteration\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(loss_history, 'b')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Current loss: %f' % loss)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using [MNIST](http://yann.lecun.com/exdb/mnist/) as our dataset. Lets start with [cool visualization](http://scs.ryerson.ca/~aharley/vis/). The most beautiful demo is the second one, if you are not familiar with convolutions you can return to it in several lectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "# Fetch MNIST dataset and create a local copy.\n",
    "if os.path.exists('mnist.npz'):\n",
    "    with np.load('mnist.npz', 'r') as data:\n",
    "        X = data['X']\n",
    "        y = data['y']\n",
    "else:\n",
    "    mnist = fetch_mldata(\"mnist-original\")\n",
    "    X, y = mnist.data / 255.0, mnist.target\n",
    "    np.savez('mnist.npz', X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the labels first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here. ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Compare** `ReLU`, `ELU` activation functions. \n",
    "You would better pick the best optimizer params for each of them, but it is overkill for now. Use an architecture of your choice for the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here. ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally**, use all your knowledge to build a super cool model on this dataset, do not forget to split dataset into train and validation. Use **dropout** to prevent overfitting, play with **learning rate decay**. You can use **data augmentation** such as rotations, translations to boost your score. Use your knowledge and imagination to train a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here. ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print here your accuracy. It should be around 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your answer goes here. ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Part: Autoencoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is **OPTIONAL**, you may not do it. It will not be scored, but it is easy and interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to build a cool model, named autoencoder. The aim is simple: **encode** the data to a lower dimentional representation. Why? Well, if we can **decode** this representation back to original data with \"small\" reconstuction loss then we can store only compressed representation saving memory. But the most important thing is -- we can reuse trained autoencoder for classification. \n",
    "<img src=\"autoencoder.png\">\n",
    "\n",
    "Picture from this [site](http://multithreaded.stitchfix.com/blog/2015/09/17/deep-style/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement an autoencoder:\n",
    "\n",
    "Build it such that dimetionality inside autoencoder changes like that: \n",
    "\n",
    "$$784 \\text{ (data)} -> 512 -> 256 -> 128 -> 30 -> 128 -> 256 -> 512 -> 784$$\n",
    "\n",
    "Use **MSECriterion** to score the reconstruction.\n",
    "\n",
    "You may train it for 9 epochs with batch size = 256, initial lr = 0.1 droping by a factor of 2 every 3 epochs. The reconstruction loss should be about 6.0 and visual quality decent already.\n",
    "Do not spend time on changing architecture, they are more or less the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here. ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some time ago NNs were a lot poorer and people were struggling to learn deep models. To train a classification net people were training autoencoder first (to train autoencoder people were pretraining single layers with [RBM](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine)), then substituting the decoder part with classification layer (yeah, they were struggling with training autoencoders a lot, and complex techniques were used at that dark times). We are going to this now, fast and easy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract inner representation for train and validation, \n",
    "# you should get (n_samples, 30) matrices\n",
    "# Your code goes here. ################################################\n",
    "\n",
    "# Now build a logistic regression or small classification net\n",
    "cnet = Sequential()\n",
    "cnet.add(Linear(30, 2))\n",
    "cnet.add(SoftMax())\n",
    "\n",
    "# Learn the weights\n",
    "# Your code goes here. ################################################\n",
    "\n",
    "# Now chop off decoder part\n",
    "# (you may need to implement `remove` method for Sequential container) \n",
    "# Your code goes here. ################################################\n",
    "\n",
    "# And add learned layers ontop.\n",
    "autoenc.add(cnet[0])\n",
    "autoenc.add(cnet[1])\n",
    "\n",
    "# Now optimize whole model\n",
    "# Your code goes here. ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What do you think, does it make sense to build real-world classifiers this way ? Did it work better for you than a straightforward one? Looks like it was not the same ~8 years ago, what has changed beside computational power? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run PCA with 30 components on the *train set*, plot original image, autoencoder and PCA reconstructions side by side for 10 samples from *validation set*.\n",
    "Probably you need to use the following snippet to make aoutpencoder examples look comparible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.clip(prediction,0,1)\n",
    "#\n",
    "# Your code goes here. ################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
