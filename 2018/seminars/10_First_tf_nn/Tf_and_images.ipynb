{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today we are going to work with neural networks and images\n",
    "** We will use tensorflow, you can install it: https://www.tensorflow.org/install/ **\n",
    "\n",
    "* `pip install tensorflow` -- **cpu-only** version for Linux & Mac OSX\n",
    "* if you want GPU support try -- `pip install tensorflow-gpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For lazy guys:)\n",
    "# !conda create -n tensorflow pip python=3.3,\n",
    "# !source activate tensorflow\n",
    "# !pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.5.0-cp34-cp34m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"img/act.png\" width=\"800\">\n",
    "\n",
    "## Neural networks convolutional layers extract features - quantitative representations of some attributes. After the extraction you can use these features for classification.\n",
    "\n",
    "## Let's look at popular architectures.\n",
    "\n",
    "### VGG\n",
    "\n",
    "<img src=\"img/vgg.png\" width=\"600\">\n",
    "\n",
    "### ResNet (Shortcut + Batch Normalization)\n",
    " \n",
    "<img src=\"img/resnet.png\" width=\"800\">\n",
    " \n",
    "### GoogleNet (Predict classes for many times)\n",
    "\n",
    " \n",
    "<img src=\"img/gln.png\" width=\"800\">\n",
    "\n",
    "\n",
    "## Deeper layer $\\to$ more complex features.\n",
    "\n",
    "<img src=\"img/feat.png\" width=\"800\">\n",
    "\n",
    "## In practice it is easier to learn pre-trained NN (Fine-Tuning)\n",
    "\n",
    "<img src=\"img/ft.jpg\" width=\"600\">\n",
    "\n",
    "## As we can see, they are really deep :)\n",
    "\n",
    "## Dark Magic \n",
    "\n",
    "<img src=\"img/dm.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vprov/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's play with tensorflow. ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions are similar to numpy. But we should remember that we declarate computational graph, and these functions will compute only while session is on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder is a constant of fix size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = tf.placeholder('int64', name=\"input_to_your_function\")\n",
    "# N square.\n",
    "result_0 = tf.pow(N,2)\n",
    "# Sum of squares of numbers up to N.\n",
    "result_1 = tf.reduce_sum((tf.range(N)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Pow:0' shape=<unknown> dtype=int64>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does it work?\n",
    "1. define placeholders where you'll send inputs;\n",
    "2. make symbolic graph: a recipe for mathematical transformation of those placeholders;\n",
    "3. compute outputs of your graph with particular values for each placeholder\n",
    "  * output.eval({placeholder:value}) \n",
    "  * s.run(output, {placeholder:value})\n",
    "\n",
    "* So far there are two main entities: \"placeholder\" and \"transformation\"\n",
    "* Both can be numbers, vectors, matrices, tensors, etc.\n",
    "* Both can be int32/64, floats of booleans (uint8) of various size.\n",
    "\n",
    "* You can define new transformations as an arbitrary operation on placeholders and other transformations\n",
    " * tf.reduce_sum(tf.arange(N)\\**2) are 3 sequential transformations of placeholder N\n",
    " * There's a tensorflow symbolic version for every numpy function\n",
    "   * `a+b, a/b, a**b, ...` behave just like in numpy\n",
    "   * np.mean -> tf.reduce_mean\n",
    "   * np.arange -> tf.range\n",
    "   * np.cumsum -> tf.cumsum\n",
    "   * If if you can't find the op you need, see the [docs](https://www.tensorflow.org/api_docs/python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All initial parameters are submited to session as dict ({Placeholder_name: value})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(result_0, {N:100}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328350\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(result_1, {N:100}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Пример посложнее.\n",
    "x = tf.placeholder(dtype='float32', shape=(10,))\n",
    "y = tf.placeholder(dtype='float32', shape=(10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "func = 100 * tf.multiply(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.  100.  400.  900. 1600. 2500. 3600. 4900. 6400. 8100.]\n"
     ]
    }
   ],
   "source": [
    "dummy = np.arange(10).astype('float32')\n",
    "print(func.eval({x:dummy, y:dummy}, session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also, it is easy to compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd26de52f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_scalar = tf.placeholder('float32')\n",
    "scalar_squared = my_scalar**2\n",
    "\n",
    "derivative = tf.gradients(scalar_squared, my_scalar)[0]\n",
    "\n",
    "x = np.linspace(-3,3)\n",
    "x_squared, x_squared_der = sess.run([scalar_squared,derivative],\n",
    "                                 {my_scalar:x})\n",
    "plt.plot(x, x_squared,label=\"x^2\")\n",
    "plt.plot(x, x_squared_der, label=\"derivative\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This way we can automatically compute gradients even for nasty functions. It is important, that this is not a numerical differentiation, there are formulas for derivatives within tensorflow, and it goes through graph vertexes and calculate the derivative of composite function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variables\n",
    "\n",
    "The inputs and transformations have no value outside function call. This isn't too comfortable if you want your model to have parameters (e.g. network weights) that are always present, but can change their value over time.\n",
    "\n",
    "Tensorflow solves this with `tf.Variable` objects.\n",
    "* You can assign variable a value at any time in your graph\n",
    "* Unlike placeholders, there's no need to explicitly pass values to variables when `s.run(...)`-ing\n",
    "* You can use variables the same way you use transformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_1 = tf.Variable(initial_value=np.ones(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial value [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#initialize variable(s) with initial values\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#evaluating shared variable (outside symbolicd graph)\n",
    "print(\"initial value\", sess.run(v_1))\n",
    "\n",
    "# within symbolic graph you use them just as any other inout or transformation, not \"get value\" needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will make logistic regression on tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y [shape - (360,)]: [0 1 0 1 0 1 0 0 1 1]\n",
      "X [shape - (360, 64)]:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = load_digits(2)\n",
    "X,y = mnist.data, mnist.target\n",
    "\n",
    "print(\"y [shape - %s]:\" % (str(y.shape)), y[:10])\n",
    "print(\"X [shape - %s]:\" % (str(X.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd248c992e8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9l+8Q5/pEyhkXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd2b4462860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0].reshape([8,8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(np.zeros((64,1)), dtype='float32')\n",
    "input_X = tf.placeholder('float32', shape=(None,64))\n",
    "input_y = tf.placeholder('float32', shape=(None,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_y = 1. / (1. + tf.exp(-1. * tf.matmul(input_X, weights)))\n",
    "# Why do we need this epsilon?\n",
    "epsilon = 0.005\n",
    "loss = -1. * (tf.reduce_sum( tf.multiply(input_y,tf.log( epsilon + predicted_y)) \n",
    "                            + tf.multiply((1 - input_y), tf.log(1 + epsilon - predicted_y)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use optimizer instead of hand-made gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.001, use_locking=True).minimize(loss, var_list=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <compile function that takes X and y, returns log loss and updates weights>\n",
    "def train_function(X, y,s=sess):    \n",
    "    X = X.reshape((batch_size,64))\n",
    "    y = y.reshape((batch_size,1))\n",
    "    s.run(optimizer, {input_X:X, input_y:y})\n",
    "    log_loss = s.run(loss, {input_X:X, input_y:y})\n",
    "    return log_loss\n",
    "    \n",
    "predict_function = lambda X: sess.run(tf.round(predicted_y), {input_X:X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "batch_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0:10.9860\n",
      "train auc: 0.5035714285714286\n",
      "test auc: 0.513157894736842\n",
      "loss at iter 1:34.5367\n",
      "train auc: 0.5\n",
      "test auc: 0.5\n",
      "loss at iter 2:51.3124\n",
      "train auc: 0.5346153846153846\n",
      "test auc: 0.5096153846153846\n",
      "loss at iter 3:0.2706\n",
      "train auc: 0.976923076923077\n",
      "test auc: 0.9711538461538461\n",
      "loss at iter 4:0.0040\n",
      "train auc: 0.9807692307692308\n",
      "test auc: 0.9807692307692308\n",
      "loss at iter 5:0.0635\n",
      "train auc: 0.9884615384615385\n",
      "test auc: 0.9903846153846154\n",
      "loss at iter 6:-0.0486\n",
      "train auc: 0.9884615384615385\n",
      "test auc: 0.9903846153846154\n",
      "loss at iter 7:-0.0947\n",
      "train auc: 0.9884615384615385\n",
      "test auc: 0.9903846153846154\n",
      "loss at iter 8:-0.0832\n",
      "train auc: 0.9884615384615385\n",
      "test auc: 0.9903846153846154\n",
      "loss at iter 9:-0.0932\n",
      "train auc: 0.9884615384615385\n",
      "test auc: 0.9903846153846154\n",
      "resulting weights:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd248bd24e0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd26e0acda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    rand_list = random.sample(range(0, X_train.shape[0]), batch_size)\n",
    "    X_sample = X_train[rand_list]\n",
    "    y_sample = y_train[rand_list]\n",
    "    loss_i = train_function(X_sample, y_sample)\n",
    "    print(\"loss at iter %i:%.4f\" % (i, loss_i))\n",
    "    print(\"train auc:\",roc_auc_score(y_train, predict_function(X_train)))\n",
    "    print(\"test auc:\",roc_auc_score(y_test, predict_function(X_test)))\n",
    "\n",
    "# Посмотрим, на какие места в картинке смотрит наш классификатор.  \n",
    "print (\"resulting weights:\")\n",
    "plt.imshow(weights.eval(session=sess).reshape([8,8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now remember that we are here fo neural networks.\n",
    "\n",
    "** More derails here: https://www.tensorflow.org/tutorials/layers **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1) Convolutional Layer #1: Applies 32 5x5 filters (extracting 5x5-pixel subregions), with ReLU activation function\n",
    "\n",
    "2) Pooling Layer #1: Performs max pooling with a 2x2 filter and stride of 2 (which specifies that pooled regions do not overlap)\n",
    "\n",
    "3) Convolutional Layer #2: Applies 64 5x5 filters, with ReLU activation function\n",
    "\n",
    "4) Pooling Layer #2: Again, performs max pooling with a 2x2 filter and stride of 2\n",
    "\n",
    "5) Dense Layer #1: 1,024 neurons, with dropout regularization rate of 0.4 (probability of 0.4 that any given element will be dropped during training)\n",
    "\n",
    "6) Dense Layer #2 (Logits Layer): 10 neurons, one for each digit target class (0–9). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training and eval data\n",
    "train_data = mnist.train.images # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (55000, 784) , target shape (55000,)\n"
     ]
    }
   ],
   "source": [
    "print('Train shape', train_data.shape, ', target shape', train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visible = train_data.reshape((train_data.shape[0], 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAACqCAYAAAA6El8nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAKQAm0AAAAAAAAAp8P88xrQx9isi1gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd241e40c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=7, figsize=(20, 20))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(visible[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Построим модель.\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=32,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=64,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Dense Layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "        # `logging_hook`.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/mnist_convnet_model', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# Create the Estimator\n",
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=\"/tmp/mnist_convnet_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 30.5 µs\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/mnist_convnet_model/model.ckpt-1003\n",
      "INFO:tensorflow:Saving checkpoints for 1004 into /tmp/mnist_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.89851, step = 1004\n",
      "INFO:tensorflow:global_step/sec: 12.9139\n",
      "INFO:tensorflow:loss = 1.65035, step = 1104 (7.748 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9727\n",
      "INFO:tensorflow:loss = 1.62144, step = 1204 (7.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7631\n",
      "INFO:tensorflow:loss = 1.35541, step = 1304 (7.834 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.2649\n",
      "INFO:tensorflow:loss = 1.23108, step = 1404 (8.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.7591\n",
      "INFO:tensorflow:loss = 1.12323, step = 1504 (8.504 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.5315\n",
      "INFO:tensorflow:loss = 0.969237, step = 1604 (7.980 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9438\n",
      "INFO:tensorflow:loss = 0.992188, step = 1704 (7.726 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9933\n",
      "INFO:tensorflow:loss = 0.896129, step = 1804 (7.699 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9895\n",
      "INFO:tensorflow:loss = 0.780635, step = 1904 (7.697 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4707\n",
      "INFO:tensorflow:loss = 0.757839, step = 2004 (8.017 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.8445\n",
      "INFO:tensorflow:loss = 0.699794, step = 2104 (9.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.5179\n",
      "INFO:tensorflow:loss = 0.612945, step = 2204 (7.988 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.8251\n",
      "INFO:tensorflow:loss = 0.518386, step = 2304 (8.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.1517\n",
      "INFO:tensorflow:loss = 0.733829, step = 2404 (8.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8236\n",
      "INFO:tensorflow:loss = 0.664837, step = 2504 (7.798 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7442\n",
      "INFO:tensorflow:loss = 0.643316, step = 2604 (7.847 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.847\n",
      "INFO:tensorflow:loss = 0.528361, step = 2704 (8.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4246\n",
      "INFO:tensorflow:loss = 0.485806, step = 2804 (8.049 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0302\n",
      "INFO:tensorflow:loss = 0.505021, step = 2904 (7.674 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0177\n",
      "INFO:tensorflow:loss = 0.459354, step = 3004 (7.681 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0393\n",
      "INFO:tensorflow:loss = 0.351384, step = 3104 (7.671 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0175\n",
      "INFO:tensorflow:loss = 0.504812, step = 3204 (7.681 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9779\n",
      "INFO:tensorflow:loss = 0.511359, step = 3304 (7.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0389\n",
      "INFO:tensorflow:loss = 0.583501, step = 3404 (7.671 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9231\n",
      "INFO:tensorflow:loss = 0.429777, step = 3504 (7.736 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0131\n",
      "INFO:tensorflow:loss = 0.505927, step = 3604 (7.686 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8498\n",
      "INFO:tensorflow:loss = 0.43069, step = 3704 (7.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0379\n",
      "INFO:tensorflow:loss = 0.38829, step = 3804 (7.672 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9512\n",
      "INFO:tensorflow:loss = 0.325429, step = 3904 (7.719 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9296\n",
      "INFO:tensorflow:loss = 0.499772, step = 4004 (7.735 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0005\n",
      "INFO:tensorflow:loss = 0.374495, step = 4104 (7.692 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.941\n",
      "INFO:tensorflow:loss = 0.349984, step = 4204 (7.729 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9674\n",
      "INFO:tensorflow:loss = 0.310932, step = 4304 (7.711 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7784\n",
      "INFO:tensorflow:loss = 0.279076, step = 4404 (7.827 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6517\n",
      "INFO:tensorflow:loss = 0.307242, step = 4504 (7.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9662\n",
      "INFO:tensorflow:loss = 0.196299, step = 4604 (7.714 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9506\n",
      "INFO:tensorflow:loss = 0.448266, step = 4704 (7.720 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8505\n",
      "INFO:tensorflow:loss = 0.327201, step = 4804 (7.782 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.007\n",
      "INFO:tensorflow:loss = 0.416738, step = 4904 (7.690 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9192\n",
      "INFO:tensorflow:loss = 0.233762, step = 5004 (7.739 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8981\n",
      "INFO:tensorflow:loss = 0.328085, step = 5104 (7.752 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9433\n",
      "INFO:tensorflow:loss = 0.417104, step = 5204 (7.728 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6449\n",
      "INFO:tensorflow:loss = 0.241331, step = 5304 (7.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6176\n",
      "INFO:tensorflow:loss = 0.363932, step = 5404 (7.925 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8337\n",
      "INFO:tensorflow:loss = 0.268502, step = 5504 (7.791 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9314\n",
      "INFO:tensorflow:loss = 0.228924, step = 5604 (7.734 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9465\n",
      "INFO:tensorflow:loss = 0.309918, step = 5704 (7.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.948\n",
      "INFO:tensorflow:loss = 0.446995, step = 5804 (7.722 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7043\n",
      "INFO:tensorflow:loss = 0.307334, step = 5904 (7.875 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.5625\n",
      "INFO:tensorflow:loss = 0.306492, step = 6004 (7.959 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7959\n",
      "INFO:tensorflow:loss = 0.268721, step = 6104 (7.812 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8332\n",
      "INFO:tensorflow:loss = 0.351746, step = 6204 (7.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9061\n",
      "INFO:tensorflow:loss = 0.2184, step = 6304 (7.748 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9852\n",
      "INFO:tensorflow:loss = 0.177249, step = 6404 (7.700 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0516\n",
      "INFO:tensorflow:loss = 0.452351, step = 6504 (7.661 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.914\n",
      "INFO:tensorflow:loss = 0.254755, step = 6604 (7.745 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9722\n",
      "INFO:tensorflow:loss = 0.316046, step = 6704 (7.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8775\n",
      "INFO:tensorflow:loss = 0.325945, step = 6804 (7.765 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9818\n",
      "INFO:tensorflow:loss = 0.227326, step = 6904 (7.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9191\n",
      "INFO:tensorflow:loss = 0.280902, step = 7004 (7.740 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.916\n",
      "INFO:tensorflow:loss = 0.292519, step = 7104 (7.745 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.5996\n",
      "INFO:tensorflow:loss = 0.255511, step = 7204 (7.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7191\n",
      "INFO:tensorflow:loss = 0.256908, step = 7304 (7.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7963\n",
      "INFO:tensorflow:loss = 0.183298, step = 7404 (7.814 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9097\n",
      "INFO:tensorflow:loss = 0.303567, step = 7504 (7.748 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9269\n",
      "INFO:tensorflow:loss = 0.256725, step = 7604 (7.735 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9484\n",
      "INFO:tensorflow:loss = 0.230007, step = 7704 (7.723 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.786\n",
      "INFO:tensorflow:loss = 0.219981, step = 7804 (7.819 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0316\n",
      "INFO:tensorflow:loss = 0.207048, step = 7904 (7.674 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8529\n",
      "INFO:tensorflow:loss = 0.220102, step = 8004 (7.781 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9284\n",
      "INFO:tensorflow:loss = 0.257394, step = 8104 (7.736 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6365\n",
      "INFO:tensorflow:loss = 0.192312, step = 8204 (7.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8569\n",
      "INFO:tensorflow:loss = 0.191089, step = 8304 (7.776 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9781\n",
      "INFO:tensorflow:loss = 0.176683, step = 8404 (7.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8092\n",
      "INFO:tensorflow:loss = 0.226209, step = 8504 (7.806 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.007\n",
      "INFO:tensorflow:loss = 0.223059, step = 8604 (7.687 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8669 into /tmp/mnist_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 12.844\n",
      "INFO:tensorflow:loss = 0.319387, step = 8704 (7.788 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6044\n",
      "INFO:tensorflow:loss = 0.109491, step = 8804 (7.935 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.3701\n",
      "INFO:tensorflow:loss = 0.145932, step = 8904 (8.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6317\n",
      "INFO:tensorflow:loss = 0.197938, step = 9004 (7.917 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 12.7952\n",
      "INFO:tensorflow:loss = 0.28596, step = 9104 (7.814 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7091\n",
      "INFO:tensorflow:loss = 0.207015, step = 9204 (7.871 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7865\n",
      "INFO:tensorflow:loss = 0.0907497, step = 9304 (7.818 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9872\n",
      "INFO:tensorflow:loss = 0.189908, step = 9404 (7.700 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4411\n",
      "INFO:tensorflow:loss = 0.229828, step = 9504 (8.038 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6925\n",
      "INFO:tensorflow:loss = 0.216801, step = 9604 (7.881 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.8007\n",
      "INFO:tensorflow:loss = 0.221563, step = 9704 (8.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.487\n",
      "INFO:tensorflow:loss = 0.199536, step = 9804 (8.006 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4682\n",
      "INFO:tensorflow:loss = 0.0977587, step = 9904 (8.022 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.5917\n",
      "INFO:tensorflow:loss = 0.210455, step = 10004 (7.940 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6033\n",
      "INFO:tensorflow:loss = 0.194531, step = 10104 (7.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6991\n",
      "INFO:tensorflow:loss = 0.257555, step = 10204 (7.875 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.81\n",
      "INFO:tensorflow:loss = 0.198705, step = 10304 (7.807 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.714\n",
      "INFO:tensorflow:loss = 0.135502, step = 10404 (7.865 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4243\n",
      "INFO:tensorflow:loss = 0.367663, step = 10504 (8.053 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8342\n",
      "INFO:tensorflow:loss = 0.145507, step = 10604 (7.786 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8302\n",
      "INFO:tensorflow:loss = 0.207972, step = 10704 (7.795 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7743\n",
      "INFO:tensorflow:loss = 0.126982, step = 10804 (7.829 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8419\n",
      "INFO:tensorflow:loss = 0.204934, step = 10904 (7.786 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9023\n",
      "INFO:tensorflow:loss = 0.2114, step = 11004 (7.752 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7687\n",
      "INFO:tensorflow:loss = 0.199492, step = 11104 (7.831 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8117\n",
      "INFO:tensorflow:loss = 0.232841, step = 11204 (7.804 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8538\n",
      "INFO:tensorflow:loss = 0.339035, step = 11304 (7.781 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6866\n",
      "INFO:tensorflow:loss = 0.216816, step = 11404 (7.883 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8821\n",
      "INFO:tensorflow:loss = 0.190056, step = 11504 (7.760 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8383\n",
      "INFO:tensorflow:loss = 0.104299, step = 11604 (7.789 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9474\n",
      "INFO:tensorflow:loss = 0.157498, step = 11704 (7.723 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9155\n",
      "INFO:tensorflow:loss = 0.147599, step = 11804 (7.743 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8464\n",
      "INFO:tensorflow:loss = 0.226005, step = 11904 (7.784 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.5079\n",
      "INFO:tensorflow:loss = 0.237199, step = 12004 (7.997 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.5659\n",
      "INFO:tensorflow:loss = 0.227875, step = 12104 (7.958 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8264\n",
      "INFO:tensorflow:loss = 0.225853, step = 12204 (7.798 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9781\n",
      "INFO:tensorflow:loss = 0.141329, step = 12304 (7.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7064\n",
      "INFO:tensorflow:loss = 0.309014, step = 12404 (7.868 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.952\n",
      "INFO:tensorflow:loss = 0.289247, step = 12504 (7.722 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8961\n",
      "INFO:tensorflow:loss = 0.108463, step = 12604 (7.753 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8217\n",
      "INFO:tensorflow:loss = 0.177539, step = 12704 (7.802 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.819\n",
      "INFO:tensorflow:loss = 0.183283, step = 12804 (7.798 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9332\n",
      "INFO:tensorflow:loss = 0.376712, step = 12904 (7.732 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8977\n",
      "INFO:tensorflow:loss = 0.112644, step = 13004 (7.755 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8703\n",
      "INFO:tensorflow:loss = 0.255392, step = 13104 (7.770 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8048\n",
      "INFO:tensorflow:loss = 0.182739, step = 13204 (7.809 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9183\n",
      "INFO:tensorflow:loss = 0.0755586, step = 13304 (7.742 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6924\n",
      "INFO:tensorflow:loss = 0.206873, step = 13404 (7.878 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6248\n",
      "INFO:tensorflow:loss = 0.1424, step = 13504 (7.920 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8036\n",
      "INFO:tensorflow:loss = 0.126291, step = 13604 (7.811 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.911\n",
      "INFO:tensorflow:loss = 0.146881, step = 13704 (7.744 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9537\n",
      "INFO:tensorflow:loss = 0.220318, step = 13804 (7.720 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8818\n",
      "INFO:tensorflow:loss = 0.222037, step = 13904 (7.764 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6606\n",
      "INFO:tensorflow:loss = 0.209262, step = 14004 (7.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4177\n",
      "INFO:tensorflow:loss = 0.0942077, step = 14104 (8.052 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7314\n",
      "INFO:tensorflow:loss = 0.173896, step = 14204 (7.854 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8887\n",
      "INFO:tensorflow:loss = 0.175815, step = 14304 (7.759 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.5766\n",
      "INFO:tensorflow:loss = 0.180951, step = 14404 (7.951 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8652\n",
      "INFO:tensorflow:loss = 0.283812, step = 14504 (7.773 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7166\n",
      "INFO:tensorflow:loss = 0.149656, step = 14604 (7.866 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7163\n",
      "INFO:tensorflow:loss = 0.146403, step = 14704 (7.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8889\n",
      "INFO:tensorflow:loss = 0.137953, step = 14804 (7.758 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9259\n",
      "INFO:tensorflow:loss = 0.217984, step = 14904 (7.736 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.899\n",
      "INFO:tensorflow:loss = 0.153224, step = 15004 (7.754 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.841\n",
      "INFO:tensorflow:loss = 0.159642, step = 15104 (7.786 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8127\n",
      "INFO:tensorflow:loss = 0.169897, step = 15204 (7.806 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7473\n",
      "INFO:tensorflow:loss = 0.110624, step = 15304 (7.843 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6527\n",
      "INFO:tensorflow:loss = 0.108508, step = 15404 (7.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9195\n",
      "INFO:tensorflow:loss = 0.103384, step = 15504 (7.740 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.937\n",
      "INFO:tensorflow:loss = 0.225876, step = 15604 (7.729 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9019\n",
      "INFO:tensorflow:loss = 0.137668, step = 15704 (7.753 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6442\n",
      "INFO:tensorflow:loss = 0.0564339, step = 15804 (7.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9025\n",
      "INFO:tensorflow:loss = 0.0909148, step = 15904 (7.750 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.907\n",
      "INFO:tensorflow:loss = 0.12912, step = 16004 (7.747 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6715\n",
      "INFO:tensorflow:loss = 0.141256, step = 16104 (7.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7059\n",
      "INFO:tensorflow:loss = 0.175626, step = 16204 (7.869 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9109\n",
      "INFO:tensorflow:loss = 0.0621087, step = 16304 (7.745 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 16322 into /tmp/mnist_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 12.6079\n",
      "INFO:tensorflow:loss = 0.251031, step = 16404 (7.932 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6924\n",
      "INFO:tensorflow:loss = 0.249404, step = 16504 (7.877 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.0557\n",
      "INFO:tensorflow:loss = 0.158537, step = 16604 (8.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.1773\n",
      "INFO:tensorflow:loss = 0.121636, step = 16704 (8.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.5208\n",
      "INFO:tensorflow:loss = 0.130576, step = 16804 (8.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1388\n",
      "INFO:tensorflow:loss = 0.126521, step = 16904 (8.976 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4634\n",
      "INFO:tensorflow:loss = 0.148723, step = 17004 (8.726 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1071\n",
      "INFO:tensorflow:loss = 0.107069, step = 17104 (9.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.0092\n",
      "INFO:tensorflow:loss = 0.11372, step = 17204 (8.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.112831, step = 17304 (8.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.412\n",
      "INFO:tensorflow:loss = 0.0434363, step = 17404 (8.059 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4618\n",
      "INFO:tensorflow:loss = 0.10587, step = 17504 (8.023 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1179\n",
      "INFO:tensorflow:loss = 0.135308, step = 17604 (8.994 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4086\n",
      "INFO:tensorflow:loss = 0.153014, step = 17704 (8.766 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4287\n",
      "INFO:tensorflow:loss = 0.114052, step = 17804 (8.046 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.1165\n",
      "INFO:tensorflow:loss = 0.283811, step = 17904 (8.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.3537\n",
      "INFO:tensorflow:loss = 0.105026, step = 18004 (8.807 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.9452\n",
      "INFO:tensorflow:loss = 0.134645, step = 18104 (8.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.7792\n",
      "INFO:tensorflow:loss = 0.283455, step = 18204 (8.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.8543\n",
      "INFO:tensorflow:loss = 0.131212, step = 18304 (8.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.3218\n",
      "INFO:tensorflow:loss = 0.0782614, step = 18404 (8.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4394\n",
      "INFO:tensorflow:loss = 0.17098, step = 18504 (8.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.3373\n",
      "INFO:tensorflow:loss = 0.213785, step = 18604 (8.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7021\n",
      "INFO:tensorflow:loss = 0.0787921, step = 18704 (7.873 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.5326\n",
      "INFO:tensorflow:loss = 0.224054, step = 18804 (7.981 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.7805\n",
      "INFO:tensorflow:loss = 0.250893, step = 18904 (8.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4284\n",
      "INFO:tensorflow:loss = 0.219046, step = 19004 (8.043 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.3831\n",
      "INFO:tensorflow:loss = 0.17906, step = 19104 (8.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.3916\n",
      "INFO:tensorflow:loss = 0.177283, step = 19204 (8.067 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4855\n",
      "INFO:tensorflow:loss = 0.0962582, step = 19304 (8.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.7245\n",
      "INFO:tensorflow:loss = 0.0835109, step = 19404 (8.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.3788\n",
      "INFO:tensorflow:loss = 0.0757294, step = 19504 (8.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.3541\n",
      "INFO:tensorflow:loss = 0.17715, step = 19604 (8.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.9387\n",
      "INFO:tensorflow:loss = 0.157473, step = 19704 (8.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.3448\n",
      "INFO:tensorflow:loss = 0.0556515, step = 19804 (8.814 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1519\n",
      "INFO:tensorflow:loss = 0.14027, step = 19904 (8.975 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4086\n",
      "INFO:tensorflow:loss = 0.100919, step = 20004 (8.757 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.2519\n",
      "INFO:tensorflow:loss = 0.147989, step = 20104 (8.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4603\n",
      "INFO:tensorflow:loss = 0.186652, step = 20204 (8.024 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6373\n",
      "INFO:tensorflow:loss = 0.113084, step = 20304 (7.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.5843\n",
      "INFO:tensorflow:loss = 0.114754, step = 20404 (7.946 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2573\n",
      "INFO:tensorflow:loss = 0.13228, step = 20504 (8.883 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.3421\n",
      "INFO:tensorflow:loss = 0.22417, step = 20604 (8.816 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.6891\n",
      "INFO:tensorflow:loss = 0.19045, step = 20704 (9.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.84655\n",
      "INFO:tensorflow:loss = 0.185088, step = 20804 (10.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5914\n",
      "INFO:tensorflow:loss = 0.197069, step = 20904 (9.440 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 21003 into /tmp/mnist_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0995097.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7ff6f42665c0>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# !!! It takes a lot of time.\n",
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": train_data},\n",
    "    y=train_labels,\n",
    "    batch_size=100,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "mnist_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-02-23-18:53:24\n",
      "INFO:tensorflow:Restoring parameters from /tmp/mnist_convnet_model/model.ckpt-21003\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-23-18:53:27\n",
      "INFO:tensorflow:Saving dict for global step 21003: accuracy = 0.9697, global_step = 21003, loss = 0.101316\n",
      "{'accuracy': 0.96969998, 'loss': 0.10131599, 'global_step': 21003}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and print results\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": eval_data},\n",
    "    y=eval_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Made by Vprov **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
